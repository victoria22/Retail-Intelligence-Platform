{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1659d1c8-5820-462a-ae3c-3f41db19035f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, types, functions as F\n",
    "from pyspark.sql.functions import col, to_timestamp, expr\n",
    "from pyspark.sql.window import Window\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2205d30-30b3-45ca-af10-d1ff5cc7f30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the credentials file\n",
    "credential_location = './keys/credentials.json'\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = 'gs://COMPOSER BUCKET/credentials.json'# composer authenticationkey\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dba48df5-54a6-47a6-96ba-cd0ff184bdb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/anaconda3/envs/spark_env/lib/python3.9/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /Users/victorianweke/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/victorianweke/.ivy2/jars\n",
      "com.google.cloud.spark#spark-bigquery-with-dependencies_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-faa87af1-65dc-49d3-826d-e23be6748e8b;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.google.cloud.spark#spark-bigquery-with-dependencies_2.12;0.32.2 in central\n",
      ":: resolution report :: resolve 73ms :: artifacts dl 2ms\n",
      "\t:: modules in use:\n",
      "\tcom.google.cloud.spark#spark-bigquery-with-dependencies_2.12;0.32.2 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   1   |   0   |   0   |   0   ||   1   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-faa87af1-65dc-49d3-826d-e23be6748e8b\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 1 already retrieved (0kB/2ms)\n",
      "25/04/14 00:14:39 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "# Initialize Spark session with BigQuery and GCS support\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"UK_Online_Retail_Data_Exploration\") \\\n",
    "    .config(\"spark.jars.packages\", \"com.google.cloud.spark:spark-bigquery-with-dependencies_2.12:0.32.2\") \\\n",
    "    .config(\"spark.jars\", \"gcs-connector-hadoop3-2.2.5.jar\") \\\n",
    "    .config(\"spark.hadoop.fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\") \\\n",
    "    .config(\"spark.hadoop.fs.AbstractFileSystem.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\") \\\n",
    "    .config(\"spark.hadoop.google.cloud.auth.service.account.json.keyfile\", credential_location) \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a6954288-b713-4440-9d40-2634ef9426f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spark = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv('gs://GCS BUCKET NAME/uk_online_retail_data/online_retail.csv') # Update with GCS Bucket name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cbcbdea2-9e32-4f41-b1ee-9a2ace879205",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "541909"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_spark.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6e58d0ac-1c00-4eed-b37f-e5ea8f6f6659",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n",
      "|index|InvoiceNo|StockCode|         Description|Quantity|   InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+-----+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n",
      "|    0|   536365|   85123A|WHITE HANGING HEA...|       6|12/1/2010 8:26|     2.55|   17850.0|United Kingdom|\n",
      "|    1|   536365|    71053| WHITE METAL LANTERN|       6|12/1/2010 8:26|     3.39|   17850.0|United Kingdom|\n",
      "|    2|   536365|   84406B|CREAM CUPID HEART...|       8|12/1/2010 8:26|     2.75|   17850.0|United Kingdom|\n",
      "|    3|   536365|   84029G|KNITTED UNION FLA...|       6|12/1/2010 8:26|     3.39|   17850.0|United Kingdom|\n",
      "|    4|   536365|   84029E|RED WOOLLY HOTTIE...|       6|12/1/2010 8:26|     3.39|   17850.0|United Kingdom|\n",
      "|    5|   536365|    22752|SET 7 BABUSHKA NE...|       2|12/1/2010 8:26|     7.65|   17850.0|United Kingdom|\n",
      "|    6|   536365|    21730|GLASS STAR FROSTE...|       6|12/1/2010 8:26|     4.25|   17850.0|United Kingdom|\n",
      "|    7|   536366|    22633|HAND WARMER UNION...|       6|12/1/2010 8:28|     1.85|   17850.0|United Kingdom|\n",
      "|    8|   536366|    22632|HAND WARMER RED P...|       6|12/1/2010 8:28|     1.85|   17850.0|United Kingdom|\n",
      "|    9|   536367|    84879|ASSORTED COLOUR B...|      32|12/1/2010 8:34|     1.69|   13047.0|United Kingdom|\n",
      "|   10|   536367|    22745|POPPY'S PLAYHOUSE...|       6|12/1/2010 8:34|      2.1|   13047.0|United Kingdom|\n",
      "|   11|   536367|    22748|POPPY'S PLAYHOUSE...|       6|12/1/2010 8:34|      2.1|   13047.0|United Kingdom|\n",
      "|   12|   536367|    22749|FELTCRAFT PRINCES...|       8|12/1/2010 8:34|     3.75|   13047.0|United Kingdom|\n",
      "|   13|   536367|    22310|IVORY KNITTED MUG...|       6|12/1/2010 8:34|     1.65|   13047.0|United Kingdom|\n",
      "|   14|   536367|    84969|BOX OF 6 ASSORTED...|       6|12/1/2010 8:34|     4.25|   13047.0|United Kingdom|\n",
      "|   15|   536367|    22623|BOX OF VINTAGE JI...|       3|12/1/2010 8:34|     4.95|   13047.0|United Kingdom|\n",
      "|   16|   536367|    22622|BOX OF VINTAGE AL...|       2|12/1/2010 8:34|     9.95|   13047.0|United Kingdom|\n",
      "|   17|   536367|    21754|HOME BUILDING BLO...|       3|12/1/2010 8:34|     5.95|   13047.0|United Kingdom|\n",
      "|   18|   536367|    21755|LOVE BUILDING BLO...|       3|12/1/2010 8:34|     5.95|   13047.0|United Kingdom|\n",
      "|   19|   536367|    21777|RECIPE BOX WITH M...|       4|12/1/2010 8:34|     7.95|   13047.0|United Kingdom|\n",
      "+-----+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aa4b4d5c-0b93-43ca-93b4-8b5f077ebe69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+---------+-----------+--------+-----------+---------+----------+-------+-----+\n",
      "|index|InvoiceNo|StockCode|Description|Quantity|InvoiceDate|UnitPrice|CustomerID|Country|count|\n",
      "+-----+---------+---------+-----------+--------+-----------+---------+----------+-------+-----+\n",
      "+-----+---------+---------+-----------+--------+-----------+---------+----------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Count duplicates\n",
    "df_spark.groupBy(df_spark.columns).count().filter(\"count > 1\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5f5e6e0f-ae07-48c6-aebe-9e3f7e448f54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 13:====================================>                     (5 + 3) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+---------+-----------+--------+-----------+---------+----------+-------+\n",
      "|index|InvoiceNo|StockCode|Description|Quantity|InvoiceDate|UnitPrice|CustomerID|Country|\n",
      "+-----+---------+---------+-----------+--------+-----------+---------+----------+-------+\n",
      "|    0|        0|        0|       1454|       0|          0|        0|    135080|      0|\n",
      "+-----+---------+---------+-----------+--------+-----------+---------+----------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Count nulls for each column\n",
    "df_spark.select([F.count(F.when(F.col(c).isNull(), c)).alias(c) for c in df_spark.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fb6f02e5-3816-48ae-b55b-d27922800be1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('index', StringType(), True), StructField('InvoiceNo', StringType(), True), StructField('StockCode', StringType(), True), StructField('Description', StringType(), True), StructField('Quantity', StringType(), True), StructField('InvoiceDate', StringType(), True), StructField('UnitPrice', StringType(), True), StructField('CustomerID', StringType(), True), StructField('Country', StringType(), True)])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_spark.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dba152f5-77bc-4768-bc64-470a3168a92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = types.StructType([\n",
    "    types.StructField(\"index\", types.IntegerType(), True),\n",
    "    types.StructField(\"InvoiceNo\", types.StringType(), True),\n",
    "    types.StructField(\"StockCode\", types.StringType(), True),\n",
    "    types.StructField(\"Description\", types.StringType(), True),\n",
    "    types.StructField(\"Quantity\", types.IntegerType(), True),\n",
    "    types.StructField(\"InvoiceDate\", types.StringType(), True),   \n",
    "    types.StructField(\"UnitPrice\", types.FloatType(), True),\n",
    "    types.StructField(\"CustomerID\", types.StringType(), True),\n",
    "    types.StructField(\"Country\", types.StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0f98cf94-0703-42d1-a94e-536d8ed58913",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data with new schema\n",
    "df_spark = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(schema) \\\n",
    "    .csv('gs://GCS BUCKET NAME/uk_online_retail_data/online_retail.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "af6c5508-efdc-4c63-b64b-76e859e9c227",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explicitly convert InvoiceDate to datetime format\n",
    "#df_spark = df_spark.withColumn(\"InvoiceDate\",  F.to_date(\"InvoiceDate\", \"MM/dd/yyyy HH:mm\"))\n",
    "\n",
    "df_spark = df_spark.withColumn(\"InvoiceDate\", expr(\"try_to_timestamp(InvoiceDate, 'M/d/yyyy H:mm')\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9717ba22-ad41-46ca-ae2a-c745df4e0a9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|index|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+-----+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|    0|   536365|   85123A|WHITE HANGING HEA...|       6|2010-12-01 08:26:00|     2.55|   17850.0|United Kingdom|\n",
      "|    1|   536365|    71053| WHITE METAL LANTERN|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n",
      "|    2|   536365|   84406B|CREAM CUPID HEART...|       8|2010-12-01 08:26:00|     2.75|   17850.0|United Kingdom|\n",
      "|    3|   536365|   84029G|KNITTED UNION FLA...|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n",
      "|    4|   536365|   84029E|RED WOOLLY HOTTIE...|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n",
      "|    5|   536365|    22752|SET 7 BABUSHKA NE...|       2|2010-12-01 08:26:00|     7.65|   17850.0|United Kingdom|\n",
      "|    6|   536365|    21730|GLASS STAR FROSTE...|       6|2010-12-01 08:26:00|     4.25|   17850.0|United Kingdom|\n",
      "|    7|   536366|    22633|HAND WARMER UNION...|       6|2010-12-01 08:28:00|     1.85|   17850.0|United Kingdom|\n",
      "|    8|   536366|    22632|HAND WARMER RED P...|       6|2010-12-01 08:28:00|     1.85|   17850.0|United Kingdom|\n",
      "|    9|   536367|    84879|ASSORTED COLOUR B...|      32|2010-12-01 08:34:00|     1.69|   13047.0|United Kingdom|\n",
      "|   10|   536367|    22745|POPPY'S PLAYHOUSE...|       6|2010-12-01 08:34:00|      2.1|   13047.0|United Kingdom|\n",
      "|   11|   536367|    22748|POPPY'S PLAYHOUSE...|       6|2010-12-01 08:34:00|      2.1|   13047.0|United Kingdom|\n",
      "|   12|   536367|    22749|FELTCRAFT PRINCES...|       8|2010-12-01 08:34:00|     3.75|   13047.0|United Kingdom|\n",
      "|   13|   536367|    22310|IVORY KNITTED MUG...|       6|2010-12-01 08:34:00|     1.65|   13047.0|United Kingdom|\n",
      "|   14|   536367|    84969|BOX OF 6 ASSORTED...|       6|2010-12-01 08:34:00|     4.25|   13047.0|United Kingdom|\n",
      "|   15|   536367|    22623|BOX OF VINTAGE JI...|       3|2010-12-01 08:34:00|     4.95|   13047.0|United Kingdom|\n",
      "|   16|   536367|    22622|BOX OF VINTAGE AL...|       2|2010-12-01 08:34:00|     9.95|   13047.0|United Kingdom|\n",
      "|   17|   536367|    21754|HOME BUILDING BLO...|       3|2010-12-01 08:34:00|     5.95|   13047.0|United Kingdom|\n",
      "|   18|   536367|    21755|LOVE BUILDING BLO...|       3|2010-12-01 08:34:00|     5.95|   13047.0|United Kingdom|\n",
      "|   19|   536367|    21777|RECIPE BOX WITH M...|       4|2010-12-01 08:34:00|     7.95|   13047.0|United Kingdom|\n",
      "+-----+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9aa11ccc-0a96-4c81-afc9-0de0277bc2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize BigQuery client\n",
    "project_id = ''  \n",
    "dataset_id = ''  \n",
    "table_id = ''  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "42906c06-9346-4ea8-bff7-3d7765a1f07c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- index: integer (nullable = true)\n",
      " |-- InvoiceNo: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- InvoiceDate: timestamp (nullable = true)\n",
      " |-- UnitPrice: float (nullable = true)\n",
      " |-- CustomerID: integer (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- Month: integer (nullable = true)\n",
      " |-- MonthName: string (nullable = true)\n",
      " |-- DayOfWeek: integer (nullable = true)\n",
      " |-- NameOfDay: string (nullable = true)\n",
      " |-- Product_Performance_TotalSales: float (nullable = true)\n",
      " |-- TotalQuantity: integer (nullable = true)\n",
      " |-- AvgProfitMargin: float (nullable = true)\n",
      " |-- SalesCategory_in_sales_final: string (nullable = true)\n",
      " |-- ProfitCategory: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Filter out negative Quantity and Sales (i.e., Quantity > 0 and Sales > 0)\n",
    "df_spark_filtered = df_spark.filter((F.col(\"Quantity\") > 0) & (F.col(\"Quantity\") * F.col(\"UnitPrice\") > 0))\n",
    "\n",
    "# Transform the Data\n",
    "# Step 1: Identify the most common CustomerID per InvoiceNo (excluding null CustomerIDs)\n",
    "window_spec_customer = Window.partitionBy(\"InvoiceNo\").orderBy(F.desc(\"count\"))\n",
    "\n",
    "most_common_customer = (df_spark_filtered\n",
    "    .filter(F.col(\"CustomerID\").isNotNull())\n",
    "    .groupBy(\"InvoiceNo\", \"CustomerID\")\n",
    "    .count()\n",
    "    .withColumn(\"rank\", F.row_number().over(window_spec_customer))\n",
    "    .filter(F.col(\"rank\") == 1)\n",
    "    .select(\"InvoiceNo\", \"CustomerID\")\n",
    ")\n",
    "\n",
    "# Rename the CustomerID column in most_common_customer to avoid ambiguity\n",
    "most_common_customer = most_common_customer.withColumnRenamed(\"CustomerID\", \"MostCommonCustomerID\")\n",
    "\n",
    "# Step 2: Join back to the original DataFrame to fill missing CustomerID values\n",
    "df_filled_customer = df_spark_filtered.alias(\"df\").join(\n",
    "    most_common_customer.alias(\"mc\"),\n",
    "    on=\"InvoiceNo\",\n",
    "    how=\"left\"\n",
    ").withColumn(\n",
    "    \"CustomerID\", \n",
    "    F.coalesce(F.col(\"df.CustomerID\"), F.col(\"mc.MostCommonCustomerID\"))\n",
    ")\n",
    "\n",
    "# Drop 'MostCommonCustomerID' as we no longer need it\n",
    "df_filled_customer = df_filled_customer.drop(\"MostCommonCustomerID\")\n",
    "\n",
    "# Option: Handle null CustomerID by setting a default value\n",
    "default_customer_id = df_spark_filtered.filter(F.col(\"CustomerID\").isNotNull()).groupBy(\"CustomerID\").count().orderBy(F.desc(\"count\")).first()[0]\n",
    "\n",
    "df_filled_customer = df_filled_customer.withColumn(\n",
    "    \"CustomerID\",\n",
    "    F.when(F.col(\"CustomerID\").isNull(), default_customer_id).otherwise(F.col(\"CustomerID\"))\n",
    ")\n",
    "\n",
    "# Step 3: Identify the most common Description per StockCode\n",
    "window_spec_description = Window.partitionBy(\"StockCode\")\n",
    "\n",
    "mode_desc = (df_spark_filtered\n",
    "    .groupBy(\"StockCode\", \"Description\")\n",
    "    .count()\n",
    "    .withColumn(\"rank\", F.row_number().over(Window.partitionBy(\"StockCode\").orderBy(F.desc(\"count\"))))\n",
    "    .filter(F.col(\"rank\") == 1)\n",
    "    .select(\"StockCode\", \"Description\")\n",
    ")\n",
    "\n",
    "# Rename Description to avoid ambiguity\n",
    "most_common_description = mode_desc.withColumnRenamed(\"Description\", \"MostCommonDescription\")\n",
    "\n",
    "# Step 4: Join back to the original DataFrame to fill missing Description values\n",
    "df_filled_final = df_filled_customer.alias(\"df\").join(\n",
    "    most_common_description.alias(\"mc\"),\n",
    "    on=\"StockCode\",\n",
    "    how=\"left\"\n",
    ").withColumn(\n",
    "    \"Description\", \n",
    "    F.coalesce(F.col(\"df.Description\"), F.col(\"mc.MostCommonDescription\"))\n",
    ")\n",
    "\n",
    "# Drop 'MostCommonDescription' as it's no longer needed\n",
    "df_filled_final = df_filled_final.drop(\"MostCommonDescription\")\n",
    "\n",
    "# Handle null Description by setting a default value\n",
    "default_description = df_spark_filtered.filter(F.col(\"Description\").isNotNull()).groupBy(\"Description\").count().orderBy(F.desc(\"count\")).first()[0]\n",
    "\n",
    "df_filled_final = df_filled_final.withColumn(\n",
    "    \"Description\",\n",
    "    F.when(F.col(\"Description\").isNull(), default_description).otherwise(F.col(\"Description\"))\n",
    ")\n",
    "\n",
    "# Step 5: Extract Date Components\n",
    "df_filled_final = df_filled_final.withColumn(\n",
    "    \"Year\", F.year(\"InvoiceDate\"))\n",
    "\n",
    "df_filled_final = df_filled_final.withColumn(\n",
    "    \"Month\", F.month(\"InvoiceDate\"))\n",
    "\n",
    "df_filled_final = df_filled_final.withColumn(\n",
    "    \"MonthName\", F.date_format(\"InvoiceDate\", \"MMMM\"))\n",
    "\n",
    "df_filled_final = df_filled_final.withColumn(\n",
    "    \"DayOfWeek\", F.dayofweek(\"InvoiceDate\"))\n",
    "\n",
    "df_filled_final = df_filled_final.withColumn(\n",
    "    \"NameOfDay\", F.date_format(\"InvoiceDate\", \"EEEE\"))\n",
    "\n",
    "# Step 6: Compute Sales Volume per Product (including Year and Country)\n",
    "df_sales = df_filled_final.withColumn(\"SalesValue\", F.col(\"Quantity\") * F.col(\"UnitPrice\"))\n",
    "\n",
    "# Filter out any negative sales\n",
    "df_sales = df_sales.filter(F.col(\"SalesValue\") > 0)\n",
    "\n",
    "sales_volume = df_sales.groupBy(\"StockCode\", \"Year\", \"Country\").agg(\n",
    "    F.sum(\"SalesValue\").alias(\"TotalSales\"),\n",
    "    F.sum(\"Quantity\").alias(\"TotalQuantity\")\n",
    ")\n",
    "\n",
    "# Step 7: Compute Profit Margin per Product (Assume Cost Price = 80% of Unit Price)\n",
    "df_profit = df_sales.withColumn(\"CostPrice\", F.col(\"UnitPrice\") * 0.8) \\\n",
    "                    .withColumn(\"ProfitMargin\", (F.col(\"UnitPrice\") - F.col(\"CostPrice\")) / F.col(\"UnitPrice\"))\n",
    "\n",
    "profit_margin = df_profit.groupBy(\"StockCode\", \"Year\", \"Country\").agg(\n",
    "    F.avg(\"ProfitMargin\").alias(\"AvgProfitMargin\"),\n",
    "    F.sum(\"SalesValue\").alias(\"TotalSales\")\n",
    ")\n",
    "\n",
    "# Rename the 'TotalSales' in profit_margin before the join to avoid ambiguity\n",
    "profit_margin = profit_margin.withColumnRenamed(\"TotalSales\", \"Profit_TotalSales\")\n",
    "\n",
    "# Step 8: Partitioned window specs for ranking\n",
    "window_spec_sales = Window.partitionBy(\"Year\", \"Country\").orderBy(F.col(\"TotalSales\").desc())\n",
    "window_spec_profit = Window.partitionBy(\"Year\", \"Country\").orderBy(F.col(\"AvgProfitMargin\").desc())\n",
    "\n",
    "# Step 8.1: Classify Sales Volume and store in `sales_volume`\n",
    "sales_volume = sales_volume.withColumn(\n",
    "    \"SalesCategory_in_sales\",  # Renaming it to avoid ambiguity later\n",
    "    F.when(F.ntile(3).over(window_spec_sales) == 1, \"Low\")\n",
    "     .when(F.ntile(3).over(window_spec_sales) == 2, \"Medium\")\n",
    "     .otherwise(\"High\")\n",
    ")\n",
    "\n",
    "# Step 8.2: Merge Sales and Profit Data\n",
    "product_performance = sales_volume.alias(\"sales\").join(\n",
    "    profit_margin.alias(\"profit\"),\n",
    "    on=[\"StockCode\", \"Year\", \"Country\"],\n",
    "    how=\"inner\"\n",
    ")\n",
    "\n",
    "# Step 8.3: Classify Profit Margin within `product_performance`\n",
    "product_performance = product_performance.withColumn(\n",
    "    \"ProfitCategory\",\n",
    "    F.when(F.ntile(3).over(window_spec_profit) == 1, \"Low\")\n",
    "     .when(F.ntile(3).over(window_spec_profit) == 2, \"Medium\")\n",
    "     .otherwise(\"High\")\n",
    ")\n",
    "\n",
    "# Rename SalesCategory_in_sales to avoid ambiguity during the final select\n",
    "product_performance = product_performance.withColumnRenamed(\"SalesCategory_in_sales\", \"SalesCategory_in_sales_final\")\n",
    "\n",
    "# Ensure the SalesCategory_in_sales column from sales_volume is propagated correctly\n",
    "product_performance = product_performance.join(\n",
    "    sales_volume.select(\"StockCode\", \"Year\", \"Country\", \"SalesCategory_in_sales\").alias(\"sales_volume\"),  # Alias to avoid conflict\n",
    "    on=[\"StockCode\", \"Year\", \"Country\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# Step 9: Merge product performance data with the main dataset\n",
    "df_final = df_filled_final.alias(\"main\").join(\n",
    "    product_performance.alias(\"performance\"),\n",
    "    on=[\n",
    "        F.col(\"main.StockCode\") == F.col(\"performance.StockCode\"),\n",
    "        F.col(\"main.Year\") == F.col(\"performance.Year\"),\n",
    "        F.col(\"main.Country\") == F.col(\"performance.Country\")\n",
    "    ],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# Step 10: Select and cast final columns\n",
    "df_final = df_final.select(\n",
    "    F.col(\"main.index\").cast(types.IntegerType()),\n",
    "    F.col(\"main.InvoiceNo\").cast(types.StringType()),\n",
    "    F.col(\"main.StockCode\").cast(types.StringType()),\n",
    "    F.col(\"main.Description\").cast(types.StringType()),\n",
    "    F.col(\"main.Quantity\").cast(types.IntegerType()),\n",
    "    F.col(\"main.InvoiceDate\").cast(types.TimestampType()),\n",
    "    F.col(\"main.UnitPrice\").cast(types.FloatType()),\n",
    "    F.col(\"main.CustomerID\").cast(types.IntegerType()),  # Changed from FloatType to IntegerType\n",
    "    F.col(\"main.Country\").cast(types.StringType()),\n",
    "    F.col(\"main.Year\").cast(types.IntegerType()),\n",
    "    F.col(\"main.Month\").cast(types.IntegerType()),\n",
    "    F.col(\"main.MonthName\").cast(types.StringType()),\n",
    "    F.col(\"main.DayOfWeek\").cast(types.IntegerType()),\n",
    "    F.col(\"main.NameOfDay\").cast(types.StringType()),\n",
    "    F.col(\"performance.Profit_TotalSales\").alias(\"Product_Performance_TotalSales\").cast(types.FloatType()),\n",
    "    F.col(\"performance.TotalQuantity\").cast(types.IntegerType()),\n",
    "    F.col(\"performance.AvgProfitMargin\").cast(types.FloatType()),\n",
    "    F.col(\"performance.SalesCategory_in_sales_final\").cast(types.StringType()),  # Correct reference to SalesCategory_in_sales\n",
    "    F.col(\"performance.ProfitCategory\").cast(types.StringType())\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Check the final schema of the dataframe\n",
    "df_final.printSchema()\n",
    "\n",
    "# Write to BigQuery\n",
    "# df_final.write.format(\"bigquery\") \\\n",
    "#     .option(\"temporaryGcsBucket\", \"\") \\\n",
    "#     .option(\"project\", project_id) \\\n",
    "#     .option(\"dataset\", dataset_id) \\\n",
    "#     .option(\"table\", table_id) \\\n",
    "#     .option(\"partitionField\", \"InvoiceDate\")    \\\n",
    "#     .option(\"clusteredFields\", \"CustomerID\")    \\\n",
    "#     .mode(\"overwrite\") \\\n",
    "#     .save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ac30bb-1689-465c-8a77-f5099b7f2858",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (spark_env)",
   "language": "python",
   "name": "spark_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
